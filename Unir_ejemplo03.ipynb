{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy3u+3gQiuJMi8gYCXYnff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcstllns/UNIR2024/blob/main/Unir_ejemplo03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZzYyPZVHVix"
      },
      "source": [
        "# Perceptron Multicapa con clasificador multicategórico\n",
        "\n",
        "Para esta práctica vamos a utilizar el conjunto de datos MNIST que consiste en imágenes de números del 0 al 9 escritas a mano y digitalizadas. Una descripción del fichero la puedes encontrar en la [wikipedia](https://en.wikipedia.org/wiki/MNIST_database).\n",
        "\n",
        "Los datos están ya almacenados en Keras, con lo que con una única función los podemos cargar y, además, están ya divididos en dos conjuntos de datos, un de entrenamiento con 60000 imágenes y otro de test con 10000 imágenes.\n",
        "\n",
        "El objetivo de la red es ser capaz de identificar correctamente el número y eso es equivalente a clasificar correctamente cada imagen. Como ahora hay 10 categorías (del 0 al 9) tendremos que usar un clasificador multicategórico y para ello vamos a usar una matriz one-hot.\n",
        "\n",
        "Los datos de entrada son imágenes en escala de grises, una matriz bidimensional de 28 x 28 en la que cada pixel va de 0 a 255. La mejor manera de trabajar con imágenes es a través de redes convolucionales pero todavía no las hemos estudiado así que vamos a vectorizar la imagen, es decir, concatenar las columnas una tras otra y formas un unico vector para cada imagen.\n",
        "\n",
        "Para procesar los datos vamos a construir un perceptron multicapa como los del ejercicio anterior pero ahora como entrada vamos a tener:\n",
        "\n",
        "__X_train__: 60000 imágenes vectorizadas y normalizadas\n",
        "\n",
        "__Y_train__: 10000 clasificaciones one hot\n",
        "\n",
        "__salida__: Un capa de salida con una neurona softmax\n",
        "\n",
        "__Función de coste__: categorical_crossentropy\n",
        "\n",
        "__Optimizador__: sgd (sthocastic gradient descent)\n",
        "\n",
        "\n",
        "Una vez ajustada la red con el training set comprobaremos la precisión de nuestra red utilizando el test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BP6MJ5IG9-e"
      },
      "source": [
        "# importamos librerías necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# versiones de los paquetes\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcxG8ACgLwKo"
      },
      "source": [
        "# Definimos funciones\n",
        "\n",
        "# esta funcion nos va a servir para ver la historia de la red\n",
        "# como ha ido cambiando el loss y el accuracy\n",
        "\n",
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Mean Square Error')\n",
        "  plt.plot(hist['epoch'], hist['loss'],'r--',\n",
        "           label='Training Error')\n",
        "  plt.plot(hist['epoch'], hist['accuracy'],'b',\n",
        "           label = 'accuracy')\n",
        "  #plt.ylim([0,20])\n",
        "  plt.legend()\n",
        "  plt.show() # un €"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsg2Lpd5HDTQ"
      },
      "source": [
        "# Los datos ya estan incorporados en Keras, los cargamos con esta función\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# visualizamos la imagen 8\n",
        "plt.imshow(x_train[8])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgluGv1-MHmN"
      },
      "source": [
        "Fíjate que en una única llamada a mnist.load_data() hemos cargado todo, los conjuntos de entrenamiento (x_train, y_train) y los conjuntos de evaluación (x_test, y_test)\n",
        "\n",
        "No te preocupes mucho, simplemente Keras ha definido la carga de esos ficheros de esa manera.\n",
        "\n",
        "La última parte del código sirve para visualizar las imágenes\n",
        "\n",
        "\n",
        "```\n",
        "# visualizamos la imagen 8\n",
        "plt.imshow(x_train[8])\n",
        "plt.show()\n",
        "```\n",
        "Si cambias el número 8 puedes ir viendo otras imágenes, tienes 60000 para elegir. Aunque aparezca en color es por la función usada imshow que las colorea, en realidad son escala de grises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tL6bkamUHn0M"
      },
      "source": [
        "# comprobamos las caracteristicas del set de entrenamiento\n",
        "print(x_train.ndim)\n",
        "print(x_train.shape)\n",
        "print(y_train.ndim)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILaGi6JDNxe6"
      },
      "source": [
        "Las dimensiones son las correctas, x es una matriz de 3 dimensiones: 60000x28x28 e y un vector de 600000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w91wnzxdNnsF"
      },
      "source": [
        "# guardamos las dimensiones en variables para hacer la red mas generica\n",
        "# y reutilizable\n",
        "\n",
        "ntrain = x_train.shape[0]\n",
        "ntest  = x_test.shape[0]\n",
        "dimf = x_train.shape[1]\n",
        "dimc = x_train.shape[2]\n",
        "\n",
        "print(\"dimensiones: \", ntrain, ntest, dimf, dimc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLrz7AcWOVJV"
      },
      "source": [
        "##Preprocesado\n",
        "\n",
        "Hay que preparar las imágenes para que entren en la red, vamos a hacer dos cosas:\n",
        "1. Vectorizarlas\n",
        "1. Normalizarlas, haciendo que vayan de 0 a 1 en vez de 0 a 255\n",
        "1. Convertir y en una matriz one hot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbBtxVf3OmJT"
      },
      "source": [
        "# Preprocesamos los datos vectorizando y normalizando las imagenes\n",
        "\n",
        "x_train = x_train.astype('float32')/255.\n",
        "x_train = x_train.reshape(ntrain, dimf*dimc)\n",
        "\n",
        "x_test = x_test.astype('float32')/255.\n",
        "x_test = x_test.reshape(ntest, dimf*dimc)\n",
        "\n",
        "# comprobamos que la dimension de x es correcta\n",
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF4VZdQPPBuf"
      },
      "source": [
        "Ahora tenemos 60000 vectores de 784 (28x28) píxeles\n",
        "\n",
        "\n",
        "Creamos con y la matriz de onehot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keQpY6s2PHey"
      },
      "source": [
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oir9PxVPdyr"
      },
      "source": [
        "# Especificamos el modelo\n",
        "# Una unica capa oculta con 10 sigmoides\n",
        "# Una capa de salida con 10 softmax, una para cada categoria\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(Dense(10, activation='sigmoid', input_shape=(dimf*dimc,)))\n",
        "model.add(Dense(10, activation = 'softmax'))\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHmjFJU4RxoE"
      },
      "source": [
        "# compilamos el modelo\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=\"sgd\", # stochastic gradient descent\n",
        "              metrics= ['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwULGARUY8Zi"
      },
      "source": [
        "# compilamos el modelo\n",
        "\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer= tf.keras.optimizers.RMSprop(lr=0.001),\n",
        "              metrics= ['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrhKIgtGR7NQ"
      },
      "source": [
        "# hacemos el entrenamiento\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=100,  batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxYK4VwySE84"
      },
      "source": [
        "El batch_size no ha sido aún explicado, simplemente úsalo así, en esencia lo que estamos haciendo es utilizar lotes de 1000 y hacer el backpropagation, así se entrena la red mucho más rápido.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFHEcDzhSkdH"
      },
      "source": [
        "# hacemos un plot para ver si aprende adecuadamente\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bbdw8veTzRG"
      },
      "source": [
        "# evaluacion final con el test_set\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('test accuracy: ', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vpwvbz0UHSY"
      },
      "source": [
        "Aproximadamente un 83% de precisión en la clasificación. Está muy bien para ser matrices vectorizadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqUxnBObVZyY"
      },
      "source": [
        "Vamos a ver la salida de la capa de neuronas softmax, la salida de la red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_bFixVoVizM"
      },
      "source": [
        "yp = model.predict(x_test)\n",
        "\n",
        "print(yp[11])\n",
        "print(np.argmax(yp[11]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIkI-133WCSk"
      },
      "source": [
        "La categoria final es la de la neurona que tenga el valor más alto. En este caso la 6 (en python se empieza a contar en 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XirSO5SWtWl"
      },
      "source": [
        "# Ejercicio\n",
        "\n",
        "Prueba a modificar los hiperparámetros de la red e incluso su arquitectura para obtener una mejor clasificación. Por ejemplo podrías hacer lo siguiente:\n",
        "\n",
        "1. Prueba a utilizar otros tipos de neuronas (RelU o tanh)\n",
        "1. Prueba a incrementar el número de capas y de neuronas\n",
        "1. Prueba a cambiar el batch_size\n",
        "1. Prueba a utilizar otros optimizadores como adam o RMSprop\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer= tf.keras.optimizers.Adam(lr=0.001),\n",
        "              metrics= ['accuracy'])\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer= tf.keras.optimizers.RMSprop(lr=0.001),\n",
        "              metrics= ['accuracy'])\n",
        "```\n",
        "\n",
        "\n",
        "Haz las pruebas que consideres interesantes e interpreta los resultados.\n",
        "\n",
        "Pega el código empleado (solo las partes que has cambiado) en la ventana de abajo\n",
        "\n",
        "Para saber cómo modificar la red o cambiar de optimizador simplemente usa google, te aportará miles de ejemplos\n",
        "\n",
        "Recuerda que los resultados tienes que subirlos a un cuestionario de moodle"
      ]
    }
  ]
}