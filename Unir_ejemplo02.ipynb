{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrIHTBUXxwNGGf86NVDF6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcstllns/UNIR2024/blob/main/Unir_ejemplo02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 02. Clasificación binaria con datos de infidelidad\n",
        "\n",
        "Se analizan unos datos obtenido de Kaggle.com sobre una encuesta de infidelidad: [enlace](https://www.kaggle.com/datasets/utkarshx27/fairs-extramarital-affairs-data)\n",
        "\n",
        "\n",
        "Las variables son:\n",
        "\n",
        "**Criterio**: affairs.\n",
        "\n",
        "**Predictoras**: gender, age,\tyearsmarried,\tchildren,\treligiousness,\teducation,\toccupation,\trating.\n",
        "\n",
        "\n",
        "La variable criterio es de tipo dicotómico 0 = no, 1 = sí por lo que el análisis convencional nos lleva a un modelo binomial. La precisión (*accuracy*) en la clasificación obtenida con este modelo es de **0.72**"
      ],
      "metadata": {
        "id": "ukhvU7qQrddh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7tQYb-l9XDO"
      },
      "outputs": [],
      "source": [
        "# Paquetes y librerías que vamos a necesitar\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "from keras.utils import plot_model\n",
        "from keras.utils import set_random_seed\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos algunas funciones que vamos a necesitar\n",
        "\n",
        "# hace un plot de la historia de ajuste\n",
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.plot(hist['epoch'], hist['accuracy'],'r--',\n",
        "           label='Training Error')\n",
        "  # plt.plot(hist['epoch'], hist['val_mse'],'b',\n",
        "  #          label = 'Validation Error')\n",
        "  plt.ylim([0,1.0])\n",
        "  plt.axhline(y=0.72, color='b', linestyle='-')\n",
        "  plt.legend()\n",
        "  plt.show() # un €\n",
        "\n",
        "# normaliza un conjunto de datos\n",
        "def norm(x, st):\n",
        "    return((x - st['mean'])/st['std'])\n",
        "\n"
      ],
      "metadata": {
        "id": "R9TQM5HQ9frT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los datos desde mi Github (mcstllns/UNIR2024)\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/mcstllns/UNIR2024/main/data-affairs.csv'\n",
        "data  = pd.read_csv(url)\n",
        "print(data.keys())\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "c41IuVNK9lSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# El fichero tiene datos perdidos y hay que eliminar las filas donde estan\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "AZNGuK8J95Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos los 4 conjuntos de datos x e y, train y test\n",
        "\n",
        "x_train = data.sample(frac=0.8, random_state=0)\n",
        "x_test = data.drop(x_train.index)\n",
        "\n",
        "y_train = x_train.pop('affairs')\n",
        "y_test = x_test.pop('affairs')"
      ],
      "metadata": {
        "id": "QwELEj4n-O9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.head()"
      ],
      "metadata": {
        "id": "LUYc6DgH-auL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.head()"
      ],
      "metadata": {
        "id": "D6PGrhSE-e2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vamos a normalizar los conjuntos de datos\n",
        "\n",
        "# calculamos los estadisticos descriptivos para x_train\n",
        "x_train_stats = x_train.describe().transpose()\n",
        "x_train_stats.head()\n",
        "\n",
        "# calculamos los estadisticos descriptivos para x_test\n",
        "x_test_stats = x_test.describe().transpose()\n",
        "x_test_stats.head()\n",
        "\n",
        "x_train_norm = norm(x_train, x_train_stats)\n",
        "x_train_norm.head()\n",
        "\n",
        "x_test_norm = norm(x_test, x_test_stats)\n",
        "x_test_norm.head()"
      ],
      "metadata": {
        "id": "JDwTDnvX-olH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comprobamos que la normalizacion ha funcionado\n",
        "\n",
        "x_train_norm.describe().transpose()"
      ],
      "metadata": {
        "id": "1O0euvXS-9fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelo --------------------------\n",
        "\n",
        "\n",
        "# Configuramos la topología\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(len(x_train_norm.keys()),)))\n",
        "model.add(Dense(4, activation = 'relu'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "dg-u6kJq_Dc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilamos el modelo, definimos los hiperparámetros\n",
        "#  - Función de pérdida: binary_crossentropy\n",
        "#  - Algoritmo de aprendizaje: Adam con lr = 0.001\n",
        "#  - Métricas: accuracy\n",
        "\n",
        "set_random_seed(1)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "OWdnB26A_VQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train_norm, y_train,\n",
        "                    epochs = 50,\n",
        "                    verbose = 1\n",
        "                    ) # para evitar que se llene toda la pantalla"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WdX-bFFZ_shq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hacemos un plot para ver si aprende adecuadamente\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "xRZUNaqDBOcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluamos el modelo con el conjunto de entrenamiento\n",
        "\n",
        "train_loss, train_acc = model.evaluate(x_train_norm, y_train)  # se evalua la precision de la red"
      ],
      "metadata": {
        "id": "oAsOydGoAG4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluamos el modelo con el conjunto de test\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test_norm, y_test)  # se evalua la precision de la red"
      ],
      "metadata": {
        "id": "GTw9VpvdCDap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos meter nuestros propios valores (normalizados) para ver la prediccion\n",
        "\n",
        "mip = pd.DataFrame(np.array([[0, 20, 1, 0, 0, 20, 1, 5]]), columns=x_train_norm.keys())\n",
        "mip_norm = norm(mip, x_test_stats)\n",
        "print(mip_norm)\n",
        "\n",
        "model.predict(mip_norm)\n"
      ],
      "metadata": {
        "id": "hxaaBWfXHC3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta estrategia de ir variando los valores de las variables predictoras y ver cómo afecta a la predicción se puede sistematizar y urilizarse para valorar la relevancia de las variables.\n",
        "\n",
        "Por ejemplo, en nuestro caso parece que el nivel educativo es altamente predictivo (**más nivel de estudios más probabilidad de ser infiel**)"
      ],
      "metadata": {
        "id": "XHV2xREqvaHR"
      }
    }
  ]
}